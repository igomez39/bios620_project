---
title: "Project I, BIOTAT 620W2022"
author: "Team Hot Pot; Xueting Tao, Isabel Gomez"
date: "2/14/2022"
output:
  rmarkdown::pdf_document:
    fig_caption: yes        
    includes:  
      in_header: my_header.tex
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, error = FALSE, warning = FALSE)
library(readxl)
library(lubridate)
library(dplyr)
library(ggplot2)
library(dplyr)
library(gridExtra)
library(GGally)
library(circular)
library(kableExtra)
```

# Abstract:  Write a high-level summary of project I.  

# **Introduction:** 



# **Data Description:** 


The data was collected between January 3rd, 2022 – February 13, 2022 (20 days) from two students in the Biostatistics 620 class is used in this study. The data was collected both from xuetao’s HUAWEI Mate 30 pro phone and isgomez's iphone. Overall following variables were collected for the study: 

* ID: the umich unique name for the individual, recorded as string type. 
* Date: Recorded everyday, recorded as date type 
* Tot.Soc.Time: Total Social Screen Time for the day, recorded in MM format 
* Pickups: Total numbers of pick-up, recorded as discrete numbers. 
* Pickup.1st: Time at first pick-up, Recorded as time type 


```{r,error = FALSE, warning=FALSE}

#read in screen time data 
screen_data = read_excel(path = "data/hotpot_screen_data.xlsx")

screen_data = screen_data %>% 
  mutate(Pickup.1st = as.POSIXct(paste(as.character(Date),
                                                   unlist(lapply(Pickup.1st,
                                                        function(x){strsplit(as.character(x), split =" ") [[1]] [2]})))))
screen_data_isgomez = screen_data %>% filter(ID == "isgomez")
screen_data_xuetao = screen_data %>% filter(ID == "xuetao")
  summary_data = data.frame(
           ID = rep(c("isgomez","xuetao","hotpot"), times = 3),
           category = c(                                         rep("tot.scr.time", times = 3),
                        rep("tot.soc.time", times = 3), 
                        rep("pickups", times = 3)), 
           # min = c(min(screen_data_isgomez$Pickup.1st),
           #         min(screen_data_xuetao$Pickup.1st),
           #         min(screen_data$Pickup.1st),
            min =  c(min(screen_data_isgomez$Tot.Scr.Time),
                   min(screen_data_xuetao$Tot.Scr.Time),
                   min(screen_data$Tot.Scr.Time),
                   min(screen_data_isgomez$Tot.Soc.Time),
                   min(screen_data_xuetao$Tot.Soc.Time),
                   min(screen_data$Tot.Soc.Time),
                   min(screen_data_isgomez$Pickups),
                   min(screen_data_xuetao$Pickups),
                   min(screen_data$Pickups)), 
           # max = c(max(screen_data_isgomez$Pickup.1st),
           #         max(screen_data_xuetao$Pickup.1st),
           #         max(screen_data$Pickup.1st),
           max =  c(max(screen_data_isgomez$Tot.Scr.Time),
                   max(screen_data_xuetao$Tot.Scr.Time),
                   max(screen_data$Tot.Scr.Time),
                   max(screen_data_isgomez$Tot.Soc.Time),
                   max(screen_data_xuetao$Tot.Soc.Time),
                   max(screen_data$Tot.Soc.Time),
                   max(screen_data_isgomez$Pickups),
                   max(screen_data_xuetao$Pickups),
                   max(screen_data$Pickups)),
           # mean = c(mean(screen_data_isgomez$Pickup.1st),
           #         mean(screen_data_xuetao$Pickup.1st),
           #         mean(screen_data$Pickup.1st),
           mean = c(mean(screen_data_isgomez$Tot.Scr.Time),
                   mean(screen_data_xuetao$Tot.Scr.Time),
                   mean(screen_data$Tot.Scr.Time),
                   mean(screen_data_isgomez$Tot.Soc.Time),
                   mean(screen_data_xuetao$Tot.Soc.Time),
                   mean(screen_data$Tot.Soc.Time),
                   mean(screen_data_isgomez$Pickups),
                   mean(screen_data_xuetao$Pickups),
                   mean(screen_data$Pickups)),
           # median = c(median(screen_data_isgomez$Pickup.1st),
           #         median(screen_data_xuetao$Pickup.1st),
           #         median(screen_data$Pickup.1st),
           median = c(median(screen_data_isgomez$Tot.Scr.Time),
                   median(screen_data_xuetao$Tot.Scr.Time),
                   median(screen_data$Tot.Scr.Time),
                   median(screen_data_isgomez$Tot.Soc.Time),
                   median(screen_data_xuetao$Tot.Soc.Time),
                   median(screen_data$Tot.Soc.Time),
                   median(screen_data_isgomez$Pickups),
                   median(screen_data_xuetao$Pickups),
                   median(screen_data$Pickups)),
           sd = c(sd(screen_data_isgomez$Tot.Scr.Time),
                   sd(screen_data_xuetao$Tot.Scr.Time),
                   sd(screen_data$Tot.Scr.Time),
                   sd(screen_data_isgomez$Tot.Soc.Time),
                   sd(screen_data_xuetao$Tot.Soc.Time),
                   sd(screen_data$Tot.Soc.Time),
                   sd(screen_data_isgomez$Pickups),
                   sd(screen_data_xuetao$Pickups),
                   sd(screen_data$Pickups)))
summary_data2 = summary_data %>% select("ID","min","max","mean","median","sd") %>% mutate(mean = round(mean,0), sd = round(sd,0), median = round(median,0))

```


```{r table1, echo=FALSE, out.width = '100%'}

kable(summary_data2, booktabs = TRUE,  caption = "Screen Data Summary Descriptive Statistic per participant") %>% pack_rows(
  index = c("Total Screen Time (in minutes)" = 3, "Total Social Time (in minutes)" = 3, "Number of daily pickups" = 3)) %>%
  kable_styling(font_size = 10, position = "center", full_width = FALSE)
```


Table 1 above shows the drastic difference in screen usage and number of pickups between the two participants in this study. As a team, the average amount of total screen time between January 3rd - February 13, 2022 is 421 minutes or approximately 7 hours. The average social screen time is 105 minutes or 1 hour and 45 minutes and the average number of daily pickups was 96.




Figure 1 displays the pairwise scatter plots of total screen time, social screen time and number of pickups for the group. There is moderately weak significant negative correlation between total screen time and number of pickups (Pearson correlation = -0.292). Social screen time has a weak positive correlation with both total screen time (pearson correlation = 0.035) and number of pickups (pearson correlation = 0.181), both of these are non-significant. 


```{r, fig.height=5, fig.width=7,fig.cap = "Pairwise scatterplot of total screen time, social screen time and total pickup for team hotpot."}
#CORRELATION PLOTS 
ggpairs(screen_data,columns = c("Tot.Scr.Time", "Tot.Soc.Time", "Pickups"),
        columnLabels = c("Total Screen Time", "Social Screen Time", "Total Pickups")) +
  theme_minimal()
```



```{r}
#getting team average screen data info
screen_data$weekday = weekdays(screen_data$Date, abbreviate = T)


# Data Description: 

screen_data = screen_data %>% group_by(Date) %>% mutate(Avg.Total.ST = mean(Tot.Scr.Time),
                                                        Avg.Social.ST = mean(Tot.Soc.Time),
                                                        Avg.Pickups = mean(Pickups))
screen_data = screen_data %>%
  mutate(if_weekend = weekday %in% c("Sun", "Sat")) 

screen_data = screen_data %>%
  group_by(weekday) %>%
  mutate(Avg.Total.ST.wkd = mean(Tot.Scr.Time),
         Avg.Social.ST.wkd = mean(Tot.Soc.Time))

screen_data$weekday  = factor(screen_data$weekday, levels=c("Sun", "Mon", "Tue",
                                        "Wed", "Thu", "Fri", "Sat"))
```

```{r, fig.height=5, fig.width=7, fig.cap = "Time Series Plots (a) - (e) team average total screen time, social screen time, team average total number of pickups, average total screen time by weekday, average social screen time by weekday"}
#total screen time 
total = ggplot(screen_data, aes(x = Date, y = Avg.Total.ST,
                        color = if_weekend)) +
        geom_line(color = "steelblue") + 
        geom_point() +
        labs(x = "", y ="Total Screen Time (min)", caption = "(a) total screen time" ) +
        ylim(15,702) +
        scale_color_manual(labels = c("weekdays", "weekends"), values = 
                            c("black","red")) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 60, hjust = 1),
          axis.title.y = element_text(size = 8, hjust = 1),
          plot.caption = element_text(hjust=0.5,vjust = 0.1, size=9),
          legend.title = element_blank())

#social screen time 
social = ggplot(screen_data, aes(x = Date, y = Avg.Social.ST,
                        color = if_weekend)) +
        geom_line(color = "steelblue") + 
        geom_point() +
        labs(x = "", y = "Social Screen Time (min)",caption = "(b) social screen time" ) +
        ylim(15,702) +
        scale_color_manual(labels = c("weekdays", "weekends"), values = 
                            c("black","red")) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 60, hjust = 1), 
          axis.title.y = element_text(size= 8, hjust = 1),
          plot.caption = element_text(hjust=0.5,vjust = 0.1, size=9),
          legend.title = element_blank())

#pickups 
pickups = ggplot(screen_data, aes(x = Date, y = Avg.Pickups,
                        color = if_weekend)) +
        geom_line(color = "steelblue") + 
        geom_point() +
        labs(x = "", y = "Total Number of Pickups)", caption = "(c) total pickups") +
        ylim(15,702) +
        scale_color_manual(labels = c("weekdays", "weekends"), values = 
                            c("black","red")) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 60, hjust = 1),
          axis.title.y = element_text(size = 7.5),
          plot.caption = element_text(hjust=0.5,vjust = 0.1, size=9),
          legend.title = element_blank())

#weekday total
weekday.total = ggplot(screen_data, aes(x = weekday, y = Avg.Total.ST.wkd)) +
  geom_col() +
  labs(caption = "(d) average total screen time", y = "Average Total Screen Time (min)") +
  theme_minimal() +
  theme(axis.title.y = element_text(size = 8),
         plot.caption = element_text(hjust=0.5,vjust = 0.1, size=9),)

#social total
social.total = ggplot(screen_data, aes(x = weekday, y = Avg.Social.ST.wkd)) +
  geom_col() +
  labs(caption = "(e) average social screen time ", y = "Average Social Screen Time (min)") +
  theme_minimal() +
  theme(axis.title.y = element_text(size = 8), 
        plot.caption = element_text(hjust=0.5,vjust = 0.1, size=9),)
  
grid.arrange(total,social,pickups,weekday.total,social.total, ncol = 2, nrow = 3)
```



```{r}
screen_data = screen_data %>% group_by(Date) %>% mutate(Avg.Pickup.1st = mean(Pickup.1st))
screen_data = screen_data %>% mutate(Pickup.1st.angular = (hour(Avg.Pickup.1st)*60+minute(Avg.Pickup.1st)) / (24*60)*360)

screen_data_isgomez = screen_data_isgomez %>% mutate(Pickup.1st.angular = (hour(Pickup.1st)*60+minute(Pickup.1st)) / (24*60)*360)
screen_data_xuetao = screen_data_xuetao %>% mutate(Pickup.1st.angular = (hour(Pickup.1st)*60+minute(Pickup.1st)) / (24*60)*360)
```

```{r, results = "hide"}
first.pickup.cir.hotpot = circular(screen_data$Pickup.1st.angular, units = "degrees", template = "clock24")

first.pickup.cir.xuetao = circular(screen_data_xuetao$Pickup.1st.angular, units = "degrees", template = "clock24")

first.pickup.cir.isgomez = circular(screen_data_isgomez$Pickup.1st.angular, units = "degrees", template = "clock24")

```


Figure 2 shows the first pick-up time for each member, along with the average of all the participants together. From this figure we can see that the most popular pickup time is around 7:30am. Whereas if we look at the individual pickup time, isgomez has a more variable pickup time with the earliest being at 8:30am and xuetao has a more consolidated pickup time around 7:30am. 
```{r, echo=FALSE,fig.cap="First pick-up time", fig.show="hold", out.width="33%"}
par(mar=c(1,1,1,1))
plot(first.pickup.cir.hotpot, stack=TRUE, bins=24, col = "blue",main="Hotpot")
plot(first.pickup.cir.isgomez, stack=TRUE, bins=24, col = "blue",main="isgomez")
plot(first.pickup.cir.xuetao, stack=TRUE, bins=24, col = "blue",main="xuetao")
```

# Data Preprocessing: 

* Since we all from the first year master student, thus we would like to define Monday to Thursday as school day and Friday to Sunday as non-school day.



# Federated Learning: 

```{r, echo=FALSE,error=FALSE,message=FALSE}

rm(list=ls(all=TRUE))  #same to clear all in stata
cat("\014")

library(xlsx)
library(dplyr)
library(lubridate)
library(stringr)
library(stringi)
library(kableExtra)
#Read in data seperately:
data_D1=read.xlsx("data/hotpot_screen_data.xlsx",sheetName ="isgomez")
data_D2=read.xlsx("data/hotpot_screen_data.xlsx",sheetName ="xuetao")

#Transfer the time into minutes:
data_D1=data_D1 %>%
  mutate(Pickup.1st.minute=(hour(Pickup.1st)*60+minute(Pickup.1st))) %>%
  mutate(pre_Tot.Scr.Time=lag(Tot.Scr.Time),
         pre_Tot.Soc.Time=lag(Tot.Soc.Time),
         X1=pre_Tot.Scr.Time,
         Schoolday=ifelse(Day %in% c("Mo","Tu","We","Th"),1,0),
         X2=Pickups,
         X1Y=X1*Pickup.1st.minute,
         X2Y=X2*Pickup.1st.minute,
         X1square=X1^2,
         X2square=X2^2,
         X1X2=X1*X2)
data_D2=data_D2 %>%
  mutate(Pickup.1st.minute=(hour(Pickup.1st)*60+minute(Pickup.1st))) %>%
  mutate(pre_Tot.Scr.Time=lag(Tot.Scr.Time),
         pre_Tot.Soc.Time=lag(Tot.Soc.Time),
         X1=pre_Tot.Scr.Time,
         Schoolday=ifelse(Day %in% c("Mo","Tu","We","Th"),1,0),
         X2=Pickups,
         X1Y=X1*Pickup.1st.minute,
         X2Y=X2*Pickup.1st.minute,
         X1square=X1^2,
         X2square=X2^2,
         X1X2=X1*X2)
data_D1=data_D1[complete.cases(data_D1),]
data_D2=data_D2[complete.cases(data_D2),]

```

Let D1 denote the data from team member 1 (Isabel) and D2 denote the data from the team member 2(Xueting). Let Y denote the time of first pick-up the following day and X denote the total screen time for the previous day. Following is the formula we used for the linear regression:

Overall Model:

$Y_i=\beta_0+\beta_1*X_{i1}+\epsilon_i$

where :

* $Y_i$ refers to First Pickup time.  
 
* $X_{i1}$ refers to Total Screen time in the previous day


From the principle of Simple linear regression, we could get following formula for the regression coefficients:

* $\hat{\beta_1}=\frac{SSXY(D1\cup D2)}{SSX(D1 \cup D2)}$ 

* $\hat{\beta_0}=\overline{Y}(D1 \cup D2)-\hat{\beta_1}*\overline{X}(D1 \cup D2)$ 

Overall SSXY and SSX could be obtained by the following way:

* $SSXY(D1 \cup D2)=(n_1+n_2)*\left \{ \overline{XY}(D1 \cup D2)-\overline{X}(D1 \cup D2)*\overline{Y}(D1 \cup D2) \right \}$ 

* $SSX(D1 \cup D2)= (n_1+n_2)*\left \{ \overline{X^2}(D1 \cup D2)-{\overline{X}}^2(D1 \cup D2)\right \}$ 

* $\overline{XY}(D1 \cup D2)=\frac{n_1}{n_1+n_2}*\overline{XY}(D1)+\frac{n_2}{n_1+n_2}*\overline{XY}(D2)$ 

* $\overline{X}(D1 \cup D2)=\frac{n_1}{n_1+n_2}*\overline{X}(D1)+\frac{n_2}{n_1+n_2}*\overline{X}(D2)$ 

* $\overline{Y}(D1 \cup D2)=\frac{n_1}{n_1+n_2}*\overline{Y}(D1)+\frac{n_2}{n_1+n_2}*\overline{Y}(D2)$ 

* $\overline{X^2}(D1 \cup D2)=\frac{n_1}{n_1+n_2}*\overline{X^2}(D1)+\frac{n_2}{n_1+n_2}*\overline{X^2}(D2)$ 

Also, we learned that the Standard Diviation for $\beta_1$ is defined as the following:

* $SD(\hat{\beta_1})=\sqrt{\hat{Var(\hat{\beta_1})}}=\sqrt{\frac{MSE(D1 \cup D2)}{SSX(D1 \cup D2)}}$ 

* $SD(\hat{\beta_0})=\sqrt{\hat{Var(\hat{\beta_0})}}=\sqrt{MSE*(\frac{1}{n}+\frac{\overline{X}^2(D1 \cup D2)}{SSX(D1 \cup D2)}})$ 

$MSE(D1 \cup D2)$ could be obtained by the following formula: 

* $MSE(D1 \cup D2)=\frac{SSE(D1)+SSE(D2)}{n_1+n_2-2}$ 

* $SSE=\sum_{i=1}^{n}(Y_i-\hat{Y_i})^2$ 

To test Whether the slope ($\beta_1$) is significant from 0, we use the following t-test:

* $\frac{\hat{\beta_1}-\beta_1}{\sqrt{\frac{MSE(D1 \cup D2)}{SSX(D1 \cup D2)}}}\sim t_{n_1+n_2-2}$ 

where $\beta_1=0$


Following is the result utiliziing the method above:

```{r, echo=FALSE,error=FALSE,message=FALSE}


n1=nrow(data_D1)
n2=nrow(data_D2)
#Indivdual mean
XY_D1_bar=mean(data_D1$X1Y,na.rm = T)
XY_D2_bar=mean(data_D2$X1Y,na.rm = T)
X_D1_bar=mean(data_D1$pre_Tot.Scr.Time,na.rm = T)
X_D2_bar=mean(data_D2$pre_Tot.Scr.Time,na.rm = T)
Y_D1_bar=mean(data_D1$Pickup.1st.minute,na.rm = T)
Y_D2_bar=mean(data_D2$Pickup.1st.minute,na.rm = T)
Xsquare_D1_bar=mean(data_D1$X1square,na.rm = T)
Xsquare_D2_bar=mean(data_D2$X1square,na.rm = T)
#Overall mean:
XY_D1D2_bar=n1/(n1+n2)*XY_D1_bar+n2/(n1+n2)*XY_D2_bar
Y_D1D2_bar=n1/(n1+n2)*Y_D1_bar+n2/(n1+n2)*Y_D2_bar
X_D1D2_bar=n1/(n1+n2)*X_D1_bar+n2/(n1+n2)*X_D2_bar
Xsquare_D1D2_bar=n1/(n1+n2)*Xsquare_D1_bar+n2/(n1+n2)*Xsquare_D2_bar
#SSX and SSXY:
SSXY_D1D2=(n1+n2)*(XY_D1D2_bar-X_D1D2_bar*Y_D1D2_bar)
SSX_D1D2=(n1+n2)*(Xsquare_D1D2_bar-(X_D1D2_bar)^2)
beta1_hat=SSXY_D1D2/SSX_D1D2
beta0_hat=Y_D1D2_bar-beta1_hat*X_D1D2_bar
#Pass the estimates back to the individual dataset to get the SSE:
data_D1=data_D1 %>%
  mutate(Yhat=beta0_hat+beta1_hat*pre_Tot.Scr.Time) %>%
  mutate(Ydiff_square=(Yhat-Pickup.1st.minute)^2)
data_D2=data_D2 %>%
  mutate(Yhat=beta0_hat+beta1_hat*pre_Tot.Scr.Time) %>%
  mutate(Ydiff_square=(Yhat-Pickup.1st.minute)^2)

#Calculated SSE seperately
SSE_D1=sum(data_D1$Ydiff_square,na.rm = T)
SSE_D2=sum(data_D2$Ydiff_square,na.rm = T)

#Calculate MSE:
MSE=(SSE_D1+SSE_D2)/(n1+n2-2)

#Calculate SE(beta1_hat):
SD_beta1_hat=sqrt(MSE/SSX_D1D2)
SD_beta0_hat=sqrt(MSE*(1/(n1+n2)+(X_D1D2_bar)^2/SSX_D1D2))

#T score and p value
beta1_tscore=beta1_hat/SD_beta1_hat
beta1_p=2*pt(beta1_tscore, n1+n2-2, lower.tail = FALSE)

beta0_tscore=beta0_hat/SD_beta0_hat
beta0_p=2*pt(beta0_tscore, n1+n2-2, lower.tail = FALSE)

#95% CI
beta1_ul=format(beta1_hat+beta1_tscore*SD_beta1_hat,digits=2)
beta1_ll=format(beta1_hat-beta1_tscore*SD_beta1_hat,digits=2)

beta0_ul=format(beta1_hat+beta0_tscore*SD_beta0_hat,digits=2)
beta0_ll=format(beta1_hat-beta0_tscore*SD_beta0_hat,digits=2)

#Output result in a table:
betalist=format(round(c(beta0_hat,beta1_hat),4))
sdlist=format(round(c(SD_beta0_hat,SD_beta1_hat),4))
tscorelist=format(round(c(beta0_tscore,beta1_tscore),4))
pvaluelist=format(round(c(beta0_p,beta1_p),3))

pvaluelist=str_replace_all(pvaluelist,"0.000","<0.001")

cilist=c(paste0(beta0_ll,",",beta0_ul),paste0(beta1_ll,",",beta1_ul))

#federal learning output table
fed_output=cbind(betalist,sdlist,tscorelist,cilist,pvaluelist)

rname=c("Intercept","Total Screen Time")
cname=c("Coefficient","SD","T-score","95%CI","P-value")
colnames(fed_output)=cname
row.names(fed_output)=rname

kable(fed_output,booktabs = T)

```

From the result above, we found that the estimated $\hat{\beta_1}$ is `r beta1_hat`, with SD=`r SD_beta1_hat`. The calculated P value is `r beta1_p` which is more than 0.05, which is non-significant. 
This results shows that there might be positive relationship between the first pick-up and the total screen time in the previous day but the relationship is not significant.

# Meta Learning: 


Overall Model:

$Y_i=\beta_0+\beta_1*X_{i1}+\beta_2*X_{i2+}\epsilon_i$

where :

* $Y_i$ refers to First Pickup time. 
* $X_{i1}$ refers to Total Screen time in the previous day 
* $X_{i2}$ refers to the Social Sreen time

From the principle of Simple linear regression, we could get following formula for the regression coefficients:

* $\hat{\beta }=(X^TX)^{-1}X^TY$ 

If we define $X_{n*3}=\begin{bmatrix} 1 & X_1^T & X_2^T \end{bmatrix}$ , $Y_{n*1}=\begin{bmatrix} Y_1 & ... & Y_n \end{bmatrix}^T$ where $X_1$, $X_2$ are row vector.

Thus, We have 

$X^TX=\begin{bmatrix}1\\ X_1\\ X_2\end{bmatrix}_{3*n}\begin{bmatrix} 1 & X_1^T & X_2^T \end{bmatrix}_{n*3}=\begin{bmatrix}n & \sum_{i=1}^{n}X_{i1} & \sum_{i=1}^{n}X_{i2} \\ \sum_{i=1}^{n}X_{i1} & \sum_{i=1}^{n}X_{i1}^2 & \sum_{i=1}^{n}X_{i1}X_{i2} \\ \sum_{i=1}^{n}X_{i2} & \sum_{i=1}^{n}X_{i1}X_{i2} & \sum_{i=1}^{n}X_{i2}^2 \end{bmatrix}$

$X^TY=\begin{bmatrix}1\\ X_1\\ X_2\end{bmatrix}_{3*n}\begin{bmatrix} Y_1 & ... & Y_n \end{bmatrix}^T_{n*1}=\begin{bmatrix} \sum_{i=1}^{n}Y_i\\ \sum_{i=1}^{n}Y_iX_{i1} \\ \sum_{i=1}^{n}Y_iX_{i2} \end{bmatrix}$

In summary:

$\hat{\beta }=\begin{bmatrix}1 & \overline{X_{1}} & \overline{X_{2}} \\ \overline{X_{1}} & \overline{X_{1}^2} & \overline{X_{1}X_{2}} \\ \overline{X_{2}} & \overline{X_{1}X_{2}} & \overline{X_{2}^2} \end{bmatrix}^{-1}\begin{bmatrix} \overline{Y}\\ \overline{YX_{1}} \\ \overline{YX_{2}}\end{bmatrix}$

Where, from Federal learning, we have :

* $\overline{YX_1}(D1 \cup D2)=\frac{n_1}{n_1+n_2}*\overline{YX_1}(D1)+\frac{n_2}{n_1+n_2}*\overline{YX_1}(D2)$  

* $\overline{YX_2}(D1 \cup D2)=\frac{n_1}{n_1+n_2}*\overline{YX_2}(D1)+\frac{n_2}{n_1+n_2}*\overline{YX_2}(D2)$ 

* $\overline{X_1}(D1 \cup D2)=\frac{n_1}{n_1+n_2}*\overline{X_1}(D1)+\frac{n_2}{n_1+n_2}*\overline{X_1}(D2)$ 

* $\overline{X_2}(D1 \cup D2)=\frac{n_1}{n_1+n_2}*\overline{X_2}(D1)+\frac{n_2}{n_1+n_2}*\overline{X_2}(D2)$ 

* $\overline{Y}(D1 \cup D2)=\frac{n_1}{n_1+n_2}*\overline{Y}(D1)+\frac{n_2}{n_1+n_2}*\overline{Y}(D2)$ 

* $\overline{X_1^2}(D1 \cup D2)=\frac{n_1}{n_1+n_2}*\overline{X_1^2}(D1)+\frac{n_2}{n_1+n_2}*\overline{X_1^2}(D2)$ 

* $\overline{X_2^2}(D1 \cup D2)=\frac{n_1}{n_1+n_2}*\overline{X_2^2}(D1)+\frac{n_2}{n_1+n_2}*\overline{X_2^2}(D2)$ 

* $\overline{X_1X_2}(D1 \cup D2)=\frac{n_1}{n_1+n_2}*\overline{X_1X_2}(D1)+\frac{n_2}{n_1+n_2}*\overline{X_1X_2}(D2)$ 


For the Variance of $\hat{\beta}$, we have the variance-covariance matrix defined as the following:

* $\hat{Var(\hat{\beta})}=\hat{\sigma^2}(X^TX)^{-1}=MSE*(X^TX)^{-1}$ 


Where

* $MSE(D1 \cup D2)=\frac{SSE(D1)+SSE(D2)}{n_1+n_2-3}$ 

* $SSE=\sum_{i=1}^{n}(Y_i-\hat{Y_i})^2$ 


To test Whether the coefficient ($\beta_1$ or $\beta_2$) is significant from 0, we use the following t-test: 

* $\frac{\hat{\beta_i}-\beta_i}{\sqrt{\hat{Var(\hat{\beta_i})}}}\sim t_{n_1+n_2-3}$ 

where $\beta_i=0$


Following is the R code implement the formula above:

```{r, echo=FALSE,error=FALSE,message=FALSE}


n1=nrow(data_D1)
n2=nrow(data_D2)
#Indivdual mean
X1Y_D1_bar=mean(data_D1$X1Y,na.rm = T)
X1Y_D2_bar=mean(data_D2$X1Y,na.rm = T)
X2Y_D1_bar=mean(data_D1$X2Y,na.rm = T)
X2Y_D2_bar=mean(data_D2$X2Y,na.rm = T)
X1_D1_bar=mean(data_D1$X1,na.rm = T)
X1_D2_bar=mean(data_D2$X1,na.rm = T)
X2_D1_bar=mean(data_D1$X2,na.rm = T)
X2_D2_bar=mean(data_D2$X2,na.rm = T)
Y_D1_bar=mean(data_D1$Pickup.1st.minute,na.rm = T)
Y_D2_bar=mean(data_D2$Pickup.1st.minute,na.rm = T)
X1square_D1_bar=mean(data_D1$X1square,na.rm = T)
X1square_D2_bar=mean(data_D2$X1square,na.rm = T)
X2square_D1_bar=mean(data_D1$X2square,na.rm = T)
X2square_D2_bar=mean(data_D2$X2square,na.rm = T)
X1X2_D1_bar=mean(data_D1$X1X2,na.rm = T)
X1X2_D2_bar=mean(data_D2$X1X2,na.rm = T)
#Overall mean:
X1Y_D1D2_bar=n1/(n1+n2)*X1Y_D1_bar+n2/(n1+n2)*X1Y_D2_bar
X2Y_D1D2_bar=n1/(n1+n2)*X2Y_D1_bar+n2/(n1+n2)*X2Y_D2_bar
Y_D1D2_bar=n1/(n1+n2)*Y_D1_bar+n2/(n1+n2)*Y_D2_bar
X1_D1D2_bar=n1/(n1+n2)*X1_D1_bar+n2/(n1+n2)*X1_D2_bar
X2_D1D2_bar=n1/(n1+n2)*X2_D1_bar+n2/(n1+n2)*X2_D2_bar
X1square_D1D2_bar=n1/(n1+n2)*X1square_D1_bar+n2/(n1+n2)*X1square_D2_bar
X2square_D1D2_bar=n1/(n1+n2)*X2square_D1_bar+n2/(n1+n2)*X2square_D2_bar
X1X2_D1D2_bar=n1/(n1+n2)*X1X2_D1_bar+n2/(n1+n2)*X1X2_D2_bar

#Matrix and beta hat
XTX_matrix=rbind(c(1,X1_D1D2_bar,X2_D1D2_bar),
                 c(X1_D1D2_bar,X1square_D1D2_bar,X1X2_D1D2_bar),
                 c(X2_D1D2_bar,X1X2_D1D2_bar,X2square_D1D2_bar))
XTX_matrix=as.matrix(XTX_matrix)
XTX_matrix_inverse=solve(XTX_matrix)
XTY_matrix=as.matrix(c(Y_D1D2_bar,X1Y_D1D2_bar,X2Y_D1D2_bar))

beta_hat=XTX_matrix_inverse %*% XTY_matrix

#MSE:
data_D1=data_D1 %>%
  mutate(yhat2=beta_hat[1,1]+beta_hat[2,1]*X1+beta_hat[3,1]*X2) %>%
  mutate(Ydiff_square2=(yhat2-Pickup.1st.minute)^2)
data_D2=data_D2 %>%
  mutate(yhat2=beta_hat[1,1]+beta_hat[2,1]*X1+beta_hat[3,1]*X2) %>%
  mutate(Ydiff_square2=(yhat2-Pickup.1st.minute)^2)

#Calculated SSE seperately
SSE_D1=sum(data_D1$Ydiff_square2,na.rm = T)
SSE_D2=sum(data_D2$Ydiff_square2,na.rm = T)

#Calculate MSE:
MSE=(SSE_D1+SSE_D2)/(n1+n2-3)

#Variance-covariance matrix:
Varhat_betahat=diag(XTX_matrix_inverse)*c(MSE)
sd_betahat=sqrt(Varhat_betahat)

#t-score:
tscorelist=beta_hat/sd_betahat
plist=2*pt(tscorelist, n1+n2-3, lower.tail = FALSE)


#95% CI
ul=format(beta_hat+tscorelist*sd_betahat,digits=2)
ll=format(beta_hat-tscorelist*sd_betahat,digits=2)


#Output result in a table:

betalist=format(round(beta_hat,4))
sdlist=format(round(sqrt(Varhat_betahat),4))
tscorelist=format(round(tscorelist,4))
pvaluelist=format(round(plist,3))
pvaluelist=str_replace_all(pvaluelist,"0.000","<0.001")

cilist=c(paste0(ul,",",ll))

#federal learning output table
meta_output=cbind(betalist,sdlist,tscorelist,cilist,pvaluelist)

rname=c("Intercept","Total Screen Time","Total Pickups")
cname=c("Coefficient","SD","T-score","95%CI","P-value")
colnames(meta_output)=cname
row.names(meta_output)=rname

kable(meta_output,booktabs = T)


```




# Confirmation analysis: 

## Confirm the analysis:

### Federal learning:

Output from the overall linear regression

```{r, echo=FALSE,error=FALSE,message=FALSE}

data_overall=read.xlsx("data/hotpot_screen_data.xlsx",sheetName ="Sheet1")

#Transfer the time into minutes:
data_overall=rbind(data_D1,data_D2)

#Regression
fit=lm(Pickup.1st.minute~pre_Tot.Scr.Time,data=data_overall)
kable(summary(fit)$coef,booktabs=T)

```

Output from the federal learning:

```{r, echo=FALSE,error=FALSE,message=FALSE}
kable(fed_output,booktabs = T)
```

The coefficients and SD calculated in the federal learning is the same as the one calculated using overall data. 


### Meta learning:

Results using the overall data:

```{r, echo=FALSE,error=FALSE,message=FALSE}
fit=lm(Pickup.1st.minute~pre_Tot.Scr.Time+Pickups,data=data_overall)
kable(summary(fit)$coef,booktabs=T)


```

Results using the federal learning method

```{r, echo=FALSE,error=FALSE,message=FALSE}
kable(meta_output,booktabs = T)
```

# Conclusion & Discussion: 






### Sensitivity analysis by individual: Strafitication


#### Federal


* isgomez

```{r, echo=FALSE,error=FALSE,message=FALSE}
fit=lm(Pickup.1st.minute~pre_Tot.Scr.Time,data=data_D1)
kable(summary(fit)$coef,booktabs=T)
```

* xuetao

```{r, echo=FALSE,error=FALSE,message=FALSE}
fit=lm(Pickup.1st.minute~pre_Tot.Scr.Time,data=data_D2)
kable(summary(fit)$coef,booktabs=T)
```



#### Meta

* isgomez

```{r, echo=FALSE,error=FALSE,message=FALSE}
fit=lm(Pickup.1st.minute~pre_Tot.Scr.Time+Pickups,data=data_D1)
kable(summary(fit)$coef,booktabs=T)
```

* xuetao

```{r, echo=FALSE,error=FALSE,message=FALSE}
fit=lm(Pickup.1st.minute~pre_Tot.Scr.Time+Pickups,data=data_D2)
kable(summary(fit)$coef,booktabs=T)
```




# Acknowledgement: 

  
# Appendix: 

   